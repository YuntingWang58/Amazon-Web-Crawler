{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GET_UserAgent():\n",
    "    uastrings = [\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.72 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10) AppleWebKit/600.1.25 (KHTML, like Gecko) Version/8.0 Safari/600.1.25\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.1.17 (KHTML, like Gecko) Version/7.1 Safari/537.85.10\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.104 Safari/537.36\"\\\n",
    "                ]\n",
    " \n",
    "    return random.choice(uastrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay() -> None:\n",
    "    time.sleep(random.uniform(5, 30))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################   Get all pagination links  #######################\n",
    "\n",
    "def get_pagination_link(url):\n",
    "    headers = {'User-Agent': GET_UA()}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, \"lxml\")\n",
    "    #  Page Number    \n",
    "    page_node = soup.select(\"ul.a-pagination\")[0].findAll('li')\n",
    "    pageNo = len(page_node) - 2\n",
    "    page_link_list = []\n",
    "    \n",
    "    if pageNo > 3:\n",
    "        # if the totoal number of pagination> 3, all middle pages will be hidden, so the number of pages is li class='a-disabled'. Page link logic should be changed.\n",
    "        pageNo = soup.select(\"ul.a-pagination li.a-disabled\")[1].get_text()\n",
    "        \n",
    "        pagination_node = soup.select(\"ul.a-pagination\")[0].findAll('a')\n",
    "        # Get the second link and change page number\n",
    "        second_page_link = pagination_node[1].get('href')\n",
    "        page_index = second_page_link.find(\"page=\") + 5\n",
    "        pg_index =  second_page_link.find(\"pg_\") + 3\n",
    "        \n",
    "        x = range(1, int(pageNo)+1)\n",
    "        for i in x:\n",
    "            page_link = second_page_link[:page_index] + str(i) + second_page_link[page_index+1:]\n",
    "            page_link = page_link[:pg_index] + str(i)\n",
    "            page_link_list.append('https://www.amazon.co.uk'+ page_link)                     \n",
    "    else: \n",
    "        # Best seller (Only for 2 paginations)\n",
    "        pagination_node = soup.select(\"ul.a-pagination\")[0].findAll('a', limit=pageNo)\n",
    "\n",
    "        for link in pagination_node:\n",
    "            page_link = link.get('href')\n",
    "            if  page_link is not None:\n",
    "                if page_link[:5] == 'https':\n",
    "                    page_link_list.append(page_link)\n",
    "                else:\n",
    "                    page_link_list.append('https://www.amazon.co.uk'+ page_link)\n",
    "                                    \n",
    "    return page_link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################   Get all product links from each pagination  #######################\n",
    "\n",
    "def get_product_link(url):\n",
    "    # add header\n",
    "#     headers = {'User-Agent': GET_UA()}\n",
    "#     headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10) AppleWebKit/600.1.25 (KHTML, like Gecko) Version/8.0 Safari/600.1.25'}\n",
    "#     headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'}\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n",
    "    content = None\n",
    "    try:\n",
    "        r = requests.get(url, headers = headers)\n",
    "        ct = r.headers['Content-Type'].lower().strip()\n",
    "        if 'text/html' in ct:\n",
    "            content = r.content\n",
    "            soup = BeautifulSoup(content, \"lxml\")\n",
    "            if len(soup.select(\"#zg-center-div li.zg-item-immersion\")) != 0 :   # Best sellers page\n",
    "                links = soup.find_all('a', {'class': 'a-link-normal a-text-normal'})\n",
    "            elif len(soup.select(\"div.s-main-slot\")) != 0 :   # Amazon deals\n",
    "                links = soup.find_all('a', {'class': 'a-link-normal a-text-normal'})[1:]\n",
    "            else :\n",
    "                links = None \n",
    "\n",
    "            link_list = []\n",
    "            \n",
    "            if links is not None:\n",
    "                for link in links:\n",
    "                    url = 'https://www.amazon.co.uk' + link.get('href')\n",
    "                    link_list.append(url)\n",
    "            else:\n",
    "                print('Cannot find product link.')\n",
    "    \n",
    "            return link_list\n",
    "    \n",
    "        else:\n",
    "            content = r.content\n",
    "            soup = None\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  Get detailed product information  #######################\n",
    "\n",
    "def get_product_data(url):\n",
    "    headers = {'User-Agent': GET_UA()}\n",
    "    content = None\n",
    "    try:\n",
    "        \n",
    "        r = requests.get(url, headers = headers)\n",
    "        ct = r.headers['Content-Type'].lower().strip()\n",
    "        if 'text/html' in ct:\n",
    "            content = r.content\n",
    "            soup = BeautifulSoup(content, \"lxml\")\n",
    "            productInfo = []\n",
    "            # 1. Title  #\n",
    "            title = soup.find('span', attrs={'id':'productTitle'})\n",
    "            if title is not None:\n",
    "                title = title.get_text().strip()\n",
    "            else: \n",
    "                title = np.nan\n",
    "            # 2. ASIN  #\n",
    "            asin = url.split('/')[5]\n",
    "            # 3. Categories  #\n",
    "            categories_node = soup.select(\"#wayfinding-breadcrumbs_container ul.a-unordered-list\")\n",
    "            if len(categories_node) != 0:\n",
    "                categories = ''\n",
    "                i = 0\n",
    "                for li in categories_node[0].findAll(\"li\"):\n",
    "                    if i % 2 == 0: # avoid extracting special character (>)\n",
    "                        categories += li.get_text().strip() + ' '\n",
    "                    i = i + 1\n",
    "            else:\n",
    "                categories = np.nan\n",
    "            # 4. Brand  #\n",
    "            brand = soup.find('a', attrs={'id':'bylineInfo'})\n",
    "            if brand is not None:\n",
    "                brand = brand.get_text().strip()\n",
    "            else: \n",
    "                brand = np.nan\n",
    "            # 5. Price  #\n",
    "            if len(soup.select(\"#priceblock_ourprice\")) != 0:            \n",
    "                price_node = soup.select(\"#priceblock_ourprice\")\n",
    "                price = price_node[0].get_text()\n",
    "            elif len(soup.select(\"#olp-upd-new-used-freeshipping-threshold span.a-size-base.a-color-price\")) != 0 :    \n",
    "                price_node = soup.select(\"#olp-upd-new-used-freeshipping-threshold span.a-size-base.a-color-price\")\n",
    "                price = price_node[0].get_text()\n",
    "            elif len(soup.select(\"#olp-upd-new-used span.a-size-base.a-color-price\")) != 0 : \n",
    "                price_node = soup.select(\"#olp-upd-new-used span.a-size-base.a-color-price\")\n",
    "                price = price_node[0].get_text()\n",
    "            else:\n",
    "                price = np.nan                \n",
    "            # delete currency\n",
    "            currency = 'Â£'\n",
    "            if currency in price:\n",
    "                price = price.replace(currency, '')\n",
    "            # 6. Colour  #\n",
    "            color_node = soup.select(\"#variation_color_name span.selection\")\n",
    "            if len(color_node) != 0:\n",
    "                color = color_node[0].get_text().strip()\n",
    "            else:\n",
    "                color = np.nan    \n",
    "            # 7. Stock  #\n",
    "            stock_node = soup.select(\"#availability span.a-size-medium\") \n",
    "            if len(stock_node) != 0:\n",
    "                stock = re.sub(r'[^\\w\\s]','', stock_node[0].get_text().strip())\n",
    "            else:\n",
    "                stock = np.nan \n",
    "            # 8. Features  #\n",
    "            features_node = soup.select(\"#feature-bullets ul.a-unordered-list\")\n",
    "            if len(features_node) != 0:\n",
    "                features = ''\n",
    "                for li in features_node[0].findAll('li'):\n",
    "                    features += li.get_text().strip() + ' '\n",
    "            else:\n",
    "                features = np.nan \n",
    "            # 9. Product Description  #\n",
    "            pd_node = soup.select(\"#productDescription\")\n",
    "            if len(pd_node) != 0:\n",
    "                pd = ''\n",
    "                for p in pd_node[0].findAll('p'):\n",
    "                    pd += p.get_text().strip() + ' '\n",
    "            else:\n",
    "                pd = np.nan \n",
    "            # 10. Customer Reviews  #\n",
    "            reviews_node = soup.select(\"#acrPopover i.a-icon-star\")\n",
    "            if len(reviews_node) != 0:\n",
    "                reviews = reviews_node[0].get_text().strip()\n",
    "            else:\n",
    "                reviews = np.nan \n",
    "            # 11. Number of Reviews  #\n",
    "            count_node = soup.select(\"#acrCustomerReviewText\")\n",
    "            if len(count_node) != 0:\n",
    "                review_count = count_node[0].get_text().split()[0]\n",
    "            else:\n",
    "                review_count = np.nan            \n",
    "            # 12. Link of Prodcut  #           \n",
    "            link = url           \n",
    "            # 13. Sizes # \n",
    "            sizes_node = soup.select(\"#native_dropdown_selected_size_name option.dropdownAvailable\")\n",
    "            if len(sizes_node) != 0:\n",
    "                sizes = sizes_node[0].get_text().strip()\n",
    "            else:\n",
    "                sizes = np.nan\n",
    "            # 14. Weight # \n",
    "            weight_node = soup.select(\"#variation_size_name span.selection\")\n",
    "            if len(weight_node) != 0:\n",
    "                weight = weight_node[0].get_text().strip()\n",
    "            else:\n",
    "                weight = np.nan\n",
    "\n",
    "            productInfo.append(title)\n",
    "            productInfo.append(asin)\n",
    "            productInfo.append(categories)\n",
    "            productInfo.append(brand)\n",
    "            productInfo.append(price)\n",
    "            productInfo.append(color)\n",
    "            productInfo.append(stock)\n",
    "            productInfo.append(features)\n",
    "            productInfo.append(pd)\n",
    "            productInfo.append(reviews)\n",
    "            productInfo.append(review_count)\n",
    "            productInfo.append(link)\n",
    "            productInfo.append(sizes)\n",
    "            productInfo.append(weight)\n",
    "\n",
    "            return productInfo\n",
    "        else:\n",
    "            content = r.content\n",
    "            soup = None\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  Get all product links (Call get_pagination_link &  get_product_link) #######################\n",
    "\n",
    "# Sports & Outdoors URL :\n",
    "# url = 'https://www.amazon.co.uk/s?i=sports&bbn=3581866031&dc&fst=as%3Aoff&qid=1599740185&ref=lp_3581866031_nr_i_3'\n",
    "\n",
    "page_link_list = get_pagination_link(url)\n",
    "page_link_with_error = []\n",
    "product_link_list = []\n",
    "for page_link in page_link_list:\n",
    "    try:\n",
    "        product_link_list += get_product_link(page_link)\n",
    "        delay()\n",
    "    except:\n",
    "        page_link_with_error.append(page_link)\n",
    "        \n",
    "print(len(product_link_list))\n",
    "print(page_link_with_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  Get all product information (Call get_product_data) #######################\n",
    "\n",
    "product_link_with_error = []\n",
    "results = []\n",
    "for product_link in product_link_list:\n",
    "    try:\n",
    "        product_data = get_product_data(product_link)\n",
    "        if product_data is not None:\n",
    "            results.append(product_data)\n",
    "        delay()\n",
    "    except:\n",
    "        product_link_with_error.append(product_link)\n",
    "df = pd.DataFrame(results,columns=['Title','ASIN','Categories','Brand','Price','Color','Stock','Features','Product Description','Customer Reviews','Number of Reviews','Product URL','Sizes','Weight'])\n",
    "df.to_csv('amazon_warehouse_Prodcut.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
